{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2p8aYcJ69DS2HAUS9VIjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoardoZappia/DeepLearning_Units_2024/blob/main/1_Learning_representations_by_back_propagating_errors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Database"
      ],
      "metadata": {
        "id": "LAxm7gaHsDPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Optimizer\n",
        "from typing import List, Union, Callable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy"
      ],
      "metadata": {
        "id": "bVWKB1yAvDDX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE_AUTODETECT: bool = True\n",
        "device: th.device = th.device(\"cuda\" if th.cuda.is_available() and DEVICE_AUTODETECT else \"cpu\")"
      ],
      "metadata": {
        "id": "QgLFA2RpBLZo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryNumbersDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.binary_numbers = self.generate_binary_numbers()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.binary_numbers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        num = self.binary_numbers[idx][0]  # Ottieni il numero binario come tensore\n",
        "        label = self.binary_numbers[idx][1]  # Ottieni l'etichetta come tensore\n",
        "        return num, label\n",
        "\n",
        "    def is_symmetric(self, num):\n",
        "        \"\"\"\n",
        "        Verifica se un numero binario è simmetrico.\n",
        "        \"\"\"\n",
        "        return int((num == num.flip(0)).all().item())\n",
        "\n",
        "    def generate_binary_numbers(self):\n",
        "        \"\"\"\n",
        "        Genera tutti i possibili numeri binari a 6 cifre.\n",
        "        \"\"\"\n",
        "        binary_numbers = []\n",
        "        for i in range(2 ** 6):  # Genera numeri da 0 a 2^6 - 1\n",
        "            binary_tensor = th.tensor([i >> d & 1 for d in range(5, -1, -1)])  # Converte in un tensore di interi\n",
        "            is_symmetric = int(self.is_symmetric(binary_tensor))  # Determina se il numero è simmetrico (1) o meno (0)\n",
        "            binary_numbers.append((binary_tensor, is_symmetric))\n",
        "        return binary_numbers\n",
        "\n",
        "# Creazione del dataset\n",
        "binary_dataset = BinaryNumbersDataset()\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_loader: DataLoader = DataLoader(\n",
        "    dataset=binary_dataset, batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "oAhjIESZTwnG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seqmlp(input_size: int, hidden_size: int, output_size: int) -> th.nn.Sequential:\n",
        "    return th.nn.Sequential(\n",
        "        th.nn.Linear(\n",
        "            in_features=input_size, out_features=hidden_size, bias=True\n",
        "        ),  # First linear transformation\n",
        "        th.nn.Sigmoid(),  # Sigmoid activation\n",
        "        th.nn.Linear(\n",
        "            hidden_size, output_size, bias=True\n",
        "        ),  # Second linear transformation\n",
        "    )"
      ],
      "metadata": {
        "id": "uCcr07zN43GG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model: th.nn.Module = seqmlp(6, 2, 1).to(device)"
      ],
      "metadata": {
        "id": "yOOzvRtG6rob"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CRITERION: Union[th.nn.Module, Callable[[th.Tensor], th.Tensor]] = (th.nn.MSELoss())"
      ],
      "metadata": {
        "id": "6kStavw_KQiw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOptimizer(Optimizer):\n",
        "    def __init__(self, params, eps=0.1, alpha=0.9):\n",
        "        defaults = dict(eps=eps, alpha=alpha)\n",
        "        super(CustomOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        for _ in range(1425):\n",
        "            # Stampare i valori dei pesi prima dell'ottimizzazione\n",
        "#            for name, param in model.named_parameters():\n",
        "#                if param.requires_grad:\n",
        "#                    print(\"Before optimization -\", name, param.data)\n",
        "\n",
        "            loss = None\n",
        "            if closure is not None:\n",
        "                loss = closure()\n",
        "\n",
        "            for group in self.param_groups:\n",
        "                eps = group['eps']\n",
        "                alpha = group['alpha']\n",
        "\n",
        "                for p in group['params']:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    grad = p.grad.data\n",
        "                    state = self.state[p]\n",
        "\n",
        "                    # Inizializza delta w a zero se non è stato ancora inizializzato\n",
        "                    if 'delta_w' not in state:\n",
        "                        state['delta_w'] = th.zeros_like(p.data)\n",
        "\n",
        "                    delta_w = state['delta_w']\n",
        "\n",
        "                    # Aggiorna delta w\n",
        "                    delta_w = -eps * grad + alpha * delta_w\n",
        "\n",
        "                    # Aggiorna i pesi\n",
        "                    p.data.add_(delta_w)\n",
        "\n",
        "                    # Salva lo stato aggiornato\n",
        "                    state['delta_w'] = delta_w\n",
        "\n",
        "            # Stampare i valori dei pesi dopo l'ottimizzazione\n",
        "#            for name, param in model.named_parameters():\n",
        "#                if param.requires_grad:\n",
        "#                    print(\"After optimization -\", name, param.data)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, th.nn.Linear):\n",
        "        th.nn.init.uniform_(module.weight, a=-0.3, b=0.3)\n",
        "        th.nn.init.zeros_(module.bias)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpyQEJswBZBO",
        "outputId": "3c7573b0-a97f-4b1a-c01a-e585093e5b2d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=6, out_features=2, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = CustomOptimizer(model.parameters(), eps=0.1, alpha=0.9)"
      ],
      "metadata": {
        "id": "22osjbwQJZ51"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_losses: List[float] = []\n",
        "eval_acc: List[float] = []\n",
        "test_acc: List[float] = []\n",
        "\n",
        "model.train()\n",
        "\n",
        "    # Loop over data\n",
        "for x, y in train_loader:\n",
        "    # Sposta ciascun batch di input e di output sul dispositivo specificato\n",
        "    x = x.float().to(device)\n",
        "    y = y.float().to(device)\n",
        "        # Forward pass + loss computation\n",
        "    yhat = model(x)\n",
        "    y = y.view(-1, 1, 1)\n",
        "    loss = CRITERION(yhat, y)\n",
        "\n",
        "        # Zero-out past gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log the loss and accuracy on the training set...\n",
        "num_elem: int = 0\n",
        "trackingmetric: float = 0\n",
        "trackingcorrect: int = 0"
      ],
      "metadata": {
        "id": "CEXcgCcIKz6I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Remember to set the model in evaluation mode before evaluating it\n",
        "\n",
        "    # Since we are just evaluating the model, we don't need to compute gradients\n",
        "with th.no_grad():\n",
        "        # ... by looping over training data again\n",
        "        for x_e, y_e in train_loader:\n",
        "            x_e, y_e = x_e.float().to(device), y_e.float().to(device)\n",
        "            modeltarget_e = model(x_e)\n",
        "            ypred_e = th.argmax(modeltarget_e, dim=1, keepdim=True)\n",
        "            trackingmetric += CRITERION(modeltarget_e, y_e).item()\n",
        "            trackingcorrect += ypred_e.eq(y_e.view_as(ypred_e)).sum().item()\n",
        "            num_elem += x_e.shape[0]\n",
        "        eval_losses.append(trackingmetric / num_elem)\n",
        "        eval_acc.append(trackingcorrect / num_elem)"
      ],
      "metadata": {
        "id": "XyZURVTwLjPR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final training loss: {eval_losses[-1]}\")\n",
        "print(f\"Final training accuracy: {eval_acc[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHrGepOyZbz1",
        "outputId": "61581fb1-e7ce-4b1d-cfe0-a51c1c68b693"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training loss: 241.4180908203125\n",
            "Final training accuracy: 0.875\n"
          ]
        }
      ]
    }
  ]
}