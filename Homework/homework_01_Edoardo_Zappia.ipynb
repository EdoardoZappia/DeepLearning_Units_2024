{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import uniform\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of all binary vectors with six elements, where a value of 1 is assigned to each symmetric vector. The primary challenge in creating the dataset lies in its compatibility with the utilized libraries. Indeed, one of the initial obstacles was understanding the appropriate format for generating the dataset to make it usable with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# Generate data: all possible 2^6 binary vectors\n",
    "data = th.tensor([[i >> d & 1 for d in range(6)] for i in range(64)], dtype=th.float32)\n",
    "target = th.tensor([int(all(data[i] == data[i].flip(0))) for i in range(64)], dtype=th.float32)\n",
    "\n",
    "symmetric_data = data[target == 1]\n",
    "non_symmetric_data = data[target == 0]\n",
    "\n",
    "# Determine the length of the minority class\n",
    "num_symmetric = symmetric_data.shape[0]\n",
    "num_non_symmetric = non_symmetric_data.shape[0]\n",
    "\n",
    "# Oversample the minority class to match the number of samples in the majority class\n",
    "oversampled_symmetric_data = symmetric_data.repeat(num_non_symmetric // num_symmetric + 1, 1)[:num_non_symmetric]\n",
    "balanced_data = th.cat([non_symmetric_data, oversampled_symmetric_data], dim=0)\n",
    "\n",
    "# Create corresponding target labels for the balanced dataset\n",
    "balanced_target = th.cat([th.zeros(num_non_symmetric), th.ones(num_non_symmetric)])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "indices = th.randperm(balanced_data.shape[0])\n",
    "balanced_data = balanced_data[indices]\n",
    "balanced_target = balanced_target[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYMMETRIC WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment involves developing a neural network where the weights are initialized symmetrically. This choice was made based on insights from the paper:\n",
    "\n",
    "\"The key property of this solution is that for a given hidden unit, weights that are symmetric about the middle of the input vector are equal in magnitude and opposite in sign. So if a symmetrical pattern is presented, both hidden units will receive a net input of 0 from the input units, and, because the hidden units have a negative bias, both will be off. In this case the output unit, having a positive bias, will be on.\"\n",
    "[Source: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.](https://bucket.ballarin.cc/papers/oth/rumelhart1986.pdf)\n",
    "\n",
    "The network was constructed as faithfully as possible to that described in the paper, precisely to verify the possibility of reproducing its results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymmetryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Initialize the weights of the first linear layer\n",
    "        self.fc1.weight.data.uniform_(-3, 3)\n",
    "        \n",
    "        # Initialize the weights of the second linear layer\n",
    "        self.fc2.weight.data.uniform_(-3, 3)\n",
    "        \n",
    "        # Symmetry in the weights of the first layer\n",
    "        with th.no_grad():\n",
    "            weights = self.fc1.weight.data\n",
    "            size = weights.size(1)\n",
    "            for i in range(size // 2):\n",
    "                weights[:, i] = -weights[:, size - i - 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)  # Sigmoid activation function\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)  # Sigmoid to output a probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the paper\n",
    "epochs = 1425 \n",
    "epsilon = 0.1  \n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primo strato: tensor([[-0.4658, -2.6870,  2.7859, -2.7859,  2.6870,  0.4658],\n",
      "        [-0.3963, -0.3970, -0.9405,  0.9405,  0.3970,  0.3963]])\n"
     ]
    }
   ],
   "source": [
    "model = SymmetryClassifier()\n",
    "criterion = th.nn.MSELoss()   # Same as the paper\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=epsilon, momentum=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1425], Loss: 0.2464\n",
      "Epoch [200/1425], Loss: 0.2309\n",
      "Epoch [300/1425], Loss: 0.2218\n",
      "Epoch [400/1425], Loss: 0.2155\n",
      "Epoch [500/1425], Loss: 0.2104\n",
      "Epoch [600/1425], Loss: 0.2047\n",
      "Epoch [700/1425], Loss: 0.1957\n",
      "Epoch [800/1425], Loss: 0.1806\n",
      "Epoch [900/1425], Loss: 0.1613\n",
      "Epoch [1000/1425], Loss: 0.1424\n",
      "Epoch [1100/1425], Loss: 0.1225\n",
      "Epoch [1200/1425], Loss: 0.1029\n",
      "Epoch [1300/1425], Loss: 0.0864\n",
      "Epoch [1400/1425], Loss: 0.0747\n"
     ]
    }
   ],
   "source": [
    "# Training loop (on the balanced dataset)\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(balanced_data).squeeze() \n",
    "    loss = criterion(outputs, balanced_target.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at every 100th step\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[-2.1843, -5.7853,  3.3429, -3.2803,  5.8479,  2.2512],\n",
      "        [-2.0021, -4.3957,  2.0563, -2.0158,  4.4361,  2.0419]],\n",
      "       requires_grad=True)\n",
      "fc1.bias Parameter containing:\n",
      "tensor([-2.0909,  1.4148], requires_grad=True)\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[-6.1428,  5.7764]], requires_grad=True)\n",
      "fc2.bias Parameter containing:\n",
      "tensor([-2.6991], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 1.0, FPR: 0.1071428582072258, TNR: 0.8928571343421936, FNR: 0.0\n",
      "Accuracy: 0.90625\n"
     ]
    }
   ],
   "source": [
    "# Assess the model testing it on the entire dataset (unbalanced)\n",
    "with th.no_grad():\n",
    "    outputs = model(data).squeeze()\n",
    "    predictions = (outputs >= 0.5).float()\n",
    "    accuracy = (predictions == target).float().mean()\n",
    "    Tpr = ((predictions == 1) & (target == 1)).float().sum()/((target == 1)).float().sum()\n",
    "    Fpr = ((predictions == 1) & (target == 0)).float().sum()/((target == 0)).float().sum()\n",
    "    Tnr = ((predictions == 0) & (target == 0)).float().sum()/((target == 0)).float().sum()\n",
    "    Fnr = ((predictions == 0) & (target == 1)).float().sum()/((target == 1)).float().sum()\n",
    "\n",
    "    print(f'TPR: {Tpr}, FPR: {Fpr}, TNR: {Tnr}, FNR: {Fnr}')\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from various metrics, the network performs well although not at the same level as described in the paper. Nevertheless, it is notable that the symmetrically initialized weights remain symmetric throughout training on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON-SYMMETRIC WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second experiment involves constructing a neural network identical to the previous one, with the only difference being the weight initialization. In this case, the weights are uniformly distributed between -3 and 3, but they are not symmetrically chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryClassifier_not_sim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymmetryClassifier_not_sim, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Initialize the weights of the first linear layer\n",
    "        self.fc1.weight.data.uniform_(-3, 3)\n",
    "        self.fc1.bias.data.zero_()  \n",
    "        # Initialize the weights of the second linear layer\n",
    "        self.fc2.weight.data.uniform_(-3, 3)\n",
    "        self.fc2.bias.data.zero_()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)  # Sigmoid activation function\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)  # Sigmoid to output a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the paper\n",
    "epochs = 1425 \n",
    "epsilon = 0.1\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_not_sim = SymmetryClassifier_not_sim()\n",
    "criterion = th.nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model_not_sim.parameters(), lr=epsilon, momentum=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1425], Loss: 0.2481\n",
      "Epoch [200/1425], Loss: 0.2354\n",
      "Epoch [300/1425], Loss: 0.2214\n",
      "Epoch [400/1425], Loss: 0.2103\n",
      "Epoch [500/1425], Loss: 0.2018\n",
      "Epoch [600/1425], Loss: 0.1954\n",
      "Epoch [700/1425], Loss: 0.1907\n",
      "Epoch [800/1425], Loss: 0.1869\n",
      "Epoch [900/1425], Loss: 0.1836\n",
      "Epoch [1000/1425], Loss: 0.1802\n",
      "Epoch [1100/1425], Loss: 0.1765\n",
      "Epoch [1200/1425], Loss: 0.1726\n",
      "Epoch [1300/1425], Loss: 0.1692\n",
      "Epoch [1400/1425], Loss: 0.1664\n"
     ]
    }
   ],
   "source": [
    "# Training loop (on balanced dataset)\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model_not_sim(balanced_data).squeeze() \n",
    "    loss = criterion(outputs, balanced_target.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at every 100th step\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[-2.0423, -0.7728,  4.8907, -4.8292, -1.3198,  2.1148],\n",
      "        [-0.0623,  4.6020, -2.8319,  2.7310,  4.6137, -0.0325]],\n",
      "       requires_grad=True)\n",
      "fc1.bias Parameter containing:\n",
      "tensor([-1.9602, -0.6527], requires_grad=True)\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[-5.4238, -3.2359]], requires_grad=True)\n",
      "fc2.bias Parameter containing:\n",
      "tensor([3.2188], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the parameters\n",
    "for name, param in model_not_sim.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noticeable that the weights of the first linear layer are semi-symmetric, while those of the second layer are not. This leads to incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 0.5, FPR: 0.1785714328289032, TNR: 0.8214285969734192, FNR: 0.5\n",
      "Accuracy: 0.78125\n"
     ]
    }
   ],
   "source": [
    "# Assess the model testing it on the entire dataset (unbalanced)\n",
    "with th.no_grad():\n",
    "    outputs = model_not_sim(data).squeeze()\n",
    "    predictions = (outputs >= 0.5).float()\n",
    "    accuracy = (predictions == target).float().mean()\n",
    "    Tpr = ((predictions == 1) & (target == 1)).float().sum()/((target == 1)).float().sum()\n",
    "    Fpr = ((predictions == 1) & (target == 0)).float().sum()/((target == 0)).float().sum()\n",
    "    Tnr = ((predictions == 0) & (target == 0)).float().sum()/((target == 0)).float().sum()\n",
    "    Fnr = ((predictions == 0) & (target == 1)).float().sum()/((target == 1)).float().sum()\n",
    "\n",
    "    print(f'TPR: {Tpr}, FPR: {Fpr}, TNR: {Tnr}, FNR: {Fnr}')\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics of this network exhibit significantly different values compared to those of the previous network. Specifically, in addition to an accuracy value of 78%, it shows very low values for both TPR and FNR, both at 50%. This outcome suggests that if the network is not initialized with symmetric weights, it fails to render all its weights symmetric within the number of epochs stated in the paper. Further tests were conducted by increasing the number of epochs even by tenfold, but the accuracy described in the paper was never achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAPER VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third experiment, the exact neural network described in the paper was reconstructed, with weights initialized to the same values as in the original study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryClassifier_paper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymmetryClassifier_paper, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "    \n",
    "        # Initialize the weights of the first linear layer with the same values of the paper\n",
    "        self.fc1.weight.data = th.tensor([[-14.2, 3.6, -7.1, 7.1, -3.6, 14.2],\n",
    "                            [14.2, -3.6, 7.2, -7.2, 3.6, -14.2]])\n",
    "        self.fc1.bias.data = th.tensor([-1.1, -1.1])\n",
    "\n",
    "        # Initialize the weights of the second linear layer with the same values of the paper\n",
    "        weights2 = self.fc2.weight.data\n",
    "        weights2 = th.tensor([[-8.8, -8.8]])\n",
    "        self.fc2.bias.data = th.tensor([6.4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)  # Sigmoid activation function\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)  # Sigmoid to output a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the paper\n",
    "epochs = 1425\n",
    "epsilon = 0.1\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paper = SymmetryClassifier_paper()\n",
    "criterion = th.nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model_paper.parameters(), lr=epsilon, momentum=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1425], Loss: 0.4971\n",
      "Epoch [200/1425], Loss: 0.4951\n",
      "Epoch [300/1425], Loss: 0.4850\n",
      "Epoch [400/1425], Loss: 0.0782\n",
      "Epoch [500/1425], Loss: 0.0334\n",
      "Epoch [600/1425], Loss: 0.0223\n",
      "Epoch [700/1425], Loss: 0.0167\n",
      "Epoch [800/1425], Loss: 0.0133\n",
      "Epoch [900/1425], Loss: 0.0110\n",
      "Epoch [1000/1425], Loss: 0.0094\n",
      "Epoch [1100/1425], Loss: 0.0082\n",
      "Epoch [1200/1425], Loss: 0.0072\n",
      "Epoch [1300/1425], Loss: 0.0065\n",
      "Epoch [1400/1425], Loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "# Training loop (on balanced dataset)\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model_paper(balanced_data).squeeze() \n",
    "    loss = criterion(outputs, balanced_target.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at every 100th step\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[-14.5961,   3.6689,  -7.3702,   7.3394,  -3.7267,  14.5730],\n",
      "        [ 14.6330,  -3.7290,   7.3828,  -7.3984,   3.6939, -14.6457]],\n",
      "       requires_grad=True)\n",
      "fc1.bias Parameter containing:\n",
      "tensor([-2.4297, -2.4741], requires_grad=True)\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[-7.0244, -7.0425]], requires_grad=True)\n",
      "fc2.bias Parameter containing:\n",
      "tensor([3.5520], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the parameters\n",
    "for name, param in model_paper.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR: 1.0, FPR: 0.0, TNR: 1.0, FNR: 0.0\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Assess the model testing it on the entire dataset (unbalanced)\n",
    "with th.no_grad():\n",
    "    outputs = model_paper(data).squeeze()\n",
    "    predictions = (outputs >= 0.5).float()\n",
    "    accuracy = (predictions == target).float().mean()\n",
    "    Tpr = ((predictions == 1) & (target == 1)).float().sum()/((target == 1)).float().sum()\n",
    "    Fpr = ((predictions == 1) & (target == 0)).float().sum()/((target == 0)).float().sum()\n",
    "    Tnr = ((predictions == 0) & (target == 0)).float().sum()/((target == 0)).float().sum()\n",
    "    Fnr = ((predictions == 0) & (target == 1)).float().sum()/((target == 1)).float().sum()\n",
    "\n",
    "    print(f'TPR: {Tpr}, FPR: {Fpr}, TNR: {Tnr}, FNR: {Fnr}')\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we were able to achieve perfect classification results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
